{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of rl-deep-q-networks.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9EjQt_o9Xf_L"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manavgarg/MOOC/blob/master/Copy_of_rl_deep_q_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EjQt_o9Xf_L"
      },
      "source": [
        "## Copyright 2019 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "oXzTW-CnXf_Q"
      },
      "source": [
        "#@title\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONq0GBqct2J-"
      },
      "source": [
        "# 4. Deep Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_lZYMFUuHJT"
      },
      "source": [
        "In this Colab, you will combine Q-learning with neural networks to create a powerful technique, called Deep Q-Learning (DQN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_Y-7HdxuFTV"
      },
      "source": [
        "## Motivation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwbwfeFot15w"
      },
      "source": [
        "In the last Colab, you learned tabular Q-learning. Your Q-table required an entry for every combination of state and action. However, for complex environments that have many states and actions, the Q table's size becomes massive. Instead of a Q-table, you can predict Q-values using a neural network. This application of deep learning to Q-learning is called DQN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqf2mYg2kM3j"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urxZETIjO0c4"
      },
      "source": [
        "Run the following cell to set up Google Analytics for the Colab. Data from  Google Analytics helps improve the Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngfeEbGgO3rN",
        "cellView": "form"
      },
      "source": [
        "#@title Set up Google Analytics for Colab\n",
        "%reset -f\n",
        "import uuid\n",
        "client_id = uuid.uuid4()\n",
        "\n",
        "import requests\n",
        "\n",
        "# Bundle up reporting into a function.\n",
        "def report_execution():\n",
        "  requests.post('https://www.google-analytics.com/collect', \n",
        "                data=('v=1'\n",
        "                      '&tid=UA-48865479-3'\n",
        "                      '&cid={}'\n",
        "                      '&t=event'\n",
        "                      '&ec=cell'            # <-- event type\n",
        "                      '&ea=execute'         # <-- event action\n",
        "                      '&el=rl-deep-q-learning'   # <-- event label\n",
        "                      '&ev=1'               # <-- event value\n",
        "                      '&an=bundled'.format(client_id)))\n",
        "\n",
        "from IPython import get_ipython\n",
        "get_ipython().events.register('post_execute', report_execution)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67cvduNXkOvl"
      },
      "source": [
        "Run the following cell to import libraries and set up the environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nksx4wVs0Dro"
      },
      "source": [
        "import gym\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "\n",
        "EPSILON_MIN = 0.01\n",
        "CHECK_SUCCESS_INTERVAL = 100\n",
        "\n",
        "env = gym.make('FrozenLake-v0')\n",
        "\n",
        "num_states = env.observation_space.n\n",
        "num_actions = env.action_space.n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC93YAHtMUTp"
      },
      "source": [
        "## Define Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrkGBnRVMUuK"
      },
      "source": [
        "You will implement the same epsilon-greedy policy environment. However, instead of storing Q-values in a table, you will use a neural network to generate the Q-values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig3VKjezKyD2"
      },
      "source": [
        "The input to the neural net is the state. In FrozenLake, represent the state using [one-hot encoding](https://developers.google.com/machine-learning/crash-course/representation/feature-engineering). For example, encode the state `10` by running the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKw7aGg6N7ve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5491fbb-8927-4a7b-98f1-6c6c0b1c233a"
      },
      "source": [
        "np.identity(num_states)[10:10+1]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmWVcBGh78Fn"
      },
      "source": [
        "Define a function to create encode states as one-hot vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1quifpKJ8Bk8"
      },
      "source": [
        "def one_hot_encode_state(state):\n",
        "  \"\"\"Args:\n",
        "       state: An integer representing the agent's state.\n",
        "     Returns:\n",
        "       A one-hot encoded vector of the input `state`.\n",
        "  \"\"\"\n",
        "  return(np.identity(num_states)[state:state+1])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28yX_0-LN2tk"
      },
      "source": [
        "The input to the neural net is a vector of length 16. The output is a vector of Q-values for each action. Since there are 4 actions, the output is a vector of length 4.\n",
        "\n",
        "Define a nonlinear neural net with 16 inputs and 4 outputs by using the TF Keras API. The neural net has these characteristics:\n",
        "\n",
        "* Uses `relu` activation function.\n",
        "* Is initialized with small positive weights. Ideally, you should use known good initial values for the weights. The initialization with positive values is a workaround.\n",
        "* Does not use biases. To understand why, suppose you used biases. Now, for an input $s_1$, the neural network predicts $Q(s_1,a_1)$ by transforming $s_1$ to $f(s_1)$. Then the output neuron for $a_1$ adds a bias, $b_{a_{1}}$,  as follows:\n",
        "$$Q(s_1,a_1) = f(s_1) + b_{a_{1}}$$\n",
        "Similarly, for state $s_2$, the prediction $Q(s_2,a_1)$ adds the same bias $b_{a_{1}}$ because the action (and thus the output neuron) remains the same:\n",
        "$$Q(s_2,a_1) = f(s_2) + b_{a_{1}}$$\n",
        "Therefore, for the same action, Q-value predictions depend on the same bias, even if the input state varies. Training to predict the Q-value for $(s_1,a_1)$ will change $b_{a_1}$. However, changing $b_{a_1}$ will change the predicted Q-values for $(s_2,a_1)$, resulting in wrong Q-values. Therefore, do not use biases.\n",
        "\n",
        "Complete the neural net definition in the following cell as described. Then run the cell. For the solution, view the next cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRmwy-7wOgo9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef4b259a-cbb8-4968-ba1d-c1aeeebba982"
      },
      "source": [
        "def define_model(learning_rate):\n",
        "  '''Returns a shallow neural net defined using tf.keras.\n",
        "  Args:\n",
        "    learning_rate: optimizer learning rate\n",
        "  Returns:\n",
        "    model: A shallow neural net defined using tf.keras input dimension equal to\n",
        "    num_states and output dimension equal to num_actions.\n",
        "  '''\n",
        "  model = keras.Sequential()\n",
        "  # === Complete this section by replacing the \"...\" with appropriate values ===\n",
        "  model.add(keras.layers.Dense(units = num_actions,\n",
        "                                input_dim = num_states,\n",
        "                                activation = 'relu',\n",
        "                                use_bias = False,\n",
        "                                # next line initializes weights with small positive values\n",
        "                                kernel_initializer = keras.initializers.RandomUniform(minval=1e-5, maxval=0.05)\n",
        "                               ))\n",
        "  # ============================================================================\n",
        "  model.compile(optimizer = keras.optimizers.SGD(lr = learning_rate),\n",
        "                loss = 'mse')\n",
        "  return(model)\n",
        "\n",
        "learning_rate = 0.1\n",
        "model = define_model(learning_rate)\n",
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 4)                 64        \n",
            "=================================================================\n",
            "Total params: 64\n",
            "Trainable params: 64\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aI4jUM14oCTN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "415d0969-90f7-45d5-f113-6954104360b5"
      },
      "source": [
        "#@title Solution (double-click to view code)\n",
        "def define_model(learning_rate):\n",
        "  '''Returns a shallow neural net defined using tf.keras.\n",
        "  Args:\n",
        "    learning_rate: optimizer learning rate\n",
        "  Returns:\n",
        "    model: A shallow neural net defined using tf.keras input dimension equal to\n",
        "    num_states and output dimension equal to num_actions.\n",
        "  '''\n",
        "  model = []\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.Dense(units = num_actions,\n",
        "                               input_dim = num_states,\n",
        "                               activation = 'relu',\n",
        "                               use_bias = False,\n",
        "                               kernel_initializer = keras.initializers.RandomUniform(minval=1e-5, maxval=0.05)\n",
        "                              ))\n",
        "  model.compile(optimizer = keras.optimizers.SGD(lr = learning_rate),\n",
        "                loss = 'mse')\n",
        "  return(model)\n",
        "\n",
        "learning_rate = 0.1\n",
        "model = define_model(learning_rate)\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 4)                 64        \n",
            "=================================================================\n",
            "Total params: 64\n",
            "Trainable params: 64\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uKEl0omCt_x"
      },
      "source": [
        "## Calculate Q-Values from Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFezuuVFQ9MZ"
      },
      "source": [
        "You can use your neural network to predict Q-values for any state. For example, predict Q-values for state 5 by running the following cell. Since your neural network has not been trained, these predicted Q-values are inaccurate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMQLS88-Q8Sk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cb32c80-4bb8-49ce-b8fd-86ed06a89ebd"
      },
      "source": [
        "model.predict(one_hot_encode_state(5))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.02012091, 0.0009727 , 0.03240754, 0.00453357]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8ujOc1lkkgy"
      },
      "source": [
        "Complete the following cell to implement a function (identical to the previous Colab) that returns an action using an epsilon greedy policy. Then run the cell. For the solution, view the next cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Kk7ZOmwKzIf"
      },
      "source": [
        "def policy_eps_greedy(env, q_values, epsilon):\n",
        "  \"\"\"Select action given Q-values using epsilon-greedy algorithm.\n",
        "  Args:\n",
        "    q_values: q_values for all possible actions from a state.\n",
        "    epsilon: Current value of epsilon used to select action using epsilon-greedy\n",
        "             algorithm.\n",
        "  Returns:\n",
        "    action: action to take from the state.\n",
        "  \"\"\"\n",
        "  # === Complete this section by replacing the \"...\" with appropriate values ===\n",
        "  if(np.random.rand() < epsilon):\n",
        "    action = env.action_space.sample()\n",
        "  else:\n",
        "     action = np.argmax(q_values)\n",
        "  # ============================================================================\n",
        "  return action"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMoJimVLpSlQ"
      },
      "source": [
        "#@title Solution (to view code, from cell's menu, select Form -> Show Code)\n",
        "def policy_eps_greedy(env, q_values, epsilon):\n",
        "  \"\"\"Select action given Q-values using epsilon-greedy algorithm.\n",
        "  Args:\n",
        "    q_values: q_values for all possible actions from a state.\n",
        "    epsilon: Current value of epsilon used to select action using epsilon-greedy\n",
        "             algorithm.\n",
        "  Returns:\n",
        "    action: action to take from the state.\n",
        "  \"\"\"\n",
        "  if(np.random.rand() < epsilon):\n",
        "    action = env.action_space.sample()\n",
        "  else:\n",
        "    action = np.argmax(q_values)\n",
        "  return action"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdYfD-PSRsWB"
      },
      "source": [
        "In deep Q-learning, the neural network replaces the Q-table. To demonstrate how, run a full training step using the neural network.\n",
        "\n",
        "First, reset the environment and calculate Q-values for the starting state:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv3eTPKESFDg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c131d623-08cf-431c-e8e4-3c469155db86"
      },
      "source": [
        "state = env.reset()\n",
        "q_values = model.predict(one_hot_encode_state(state))\n",
        "print(\"Q-values for state \" + str(state) + \" :\\n\" + str(q_values))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-values for state 0 :\n",
            "[[0.03686138 0.00704839 0.02517122 0.01055484]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euFA4Rs0DU6i"
      },
      "source": [
        "Each Q-value represents the approximated return from taking the corresponding action and then following a greedy policy. Therefore, when Q-values are accurate, choosing the action with the highest Q-value will maximize return.\n",
        "\n",
        "Using the Q-values, select an action using an epsilon-greedy policy. Take the action and record the next state and reward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WpRbnLzDQdL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7caf9f66-a647-409b-d300-50f4504dacbc"
      },
      "source": [
        "epsilon = 0.5 # assume some value of epsilon\n",
        "\n",
        "action = policy_eps_greedy(env, q_values, epsilon)\n",
        "state_new, reward, _, _ = env.step(action)\n",
        "\n",
        "print(\"action:\", action, \", next state:\", state_new, \", reward:\", reward)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action: 3 , next state: 0 , reward: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_carvIRkXDpB"
      },
      "source": [
        "Calculate the target Q-value by completing the following cell to define a function. The formula for the returned Q-value is:\n",
        "\n",
        "$$\n",
        "  r(s,a)\n",
        "      + \\gamma \\displaystyle \\max_{\\substack{a_1}} Q(s_1,a_1)\n",
        "$$\n",
        "\n",
        "This function is similar to the Bellman update in the previous Colab, except for the use of a neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o7bIJuHXH8P"
      },
      "source": [
        "def bellman_update(reward, discount_factor, model, state_new):\n",
        "  # =========== Complete this section by replacing the \"...\" ===================\n",
        "  return reward + discount_factor*(np.max(model.predict(one_hot_encode_state(state_new))))\n",
        "  # ============================================================================"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc4WCwaLpyNI"
      },
      "source": [
        "#@title Solution (to view code, from cell's menu, select Form -> Show Code)\n",
        "def bellman_update(reward, discount_factor, model, state_new):\n",
        "  return reward + discount_factor * \\\n",
        "                  np.max(model.predict(one_hot_encode_state(state_new)))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEAOTJ5TEURA"
      },
      "source": [
        "Calculate target Q-values by calling the function `bellman_update`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dqiXnSDEUhH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a75e334-81c8-44c2-a503-90f1ca9cf586"
      },
      "source": [
        "discount_factor = 0.99\n",
        "\n",
        "print(\"Q-values before update for state \" + str(state) + \" :\\n\" + str(q_values))\n",
        "target_q_values = q_values\n",
        "target_q_values[0, action] = bellman_update(reward, discount_factor, model,\n",
        "                                           state_new)\n",
        "\n",
        "print(\"Q-values after update for state \" + str(state) + \" :\\n\" + str(target_q_values))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-values before update for state 0 :\n",
            "[[0.03686138 0.00704839 0.02517122 0.01055484]]\n",
            "Q-values after update for state 0 :\n",
            "[[0.03686138 0.00704839 0.02517122 0.03649277]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxck8BVVFFIo"
      },
      "source": [
        "Notice that only the Q-value corresponding to the action taken changes after the update. The updated Q-values become the \"target\" label that the neural network uses to train.\n",
        "\n",
        "Train the neural network to predict the target Q-values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRBdtUtWFFS7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19399e6f-a1a2-40db-f033-75a723294d28"
      },
      "source": [
        "model.fit(one_hot_encode_state(state), target_q_values, verbose = True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 374ms/step - loss: 1.6819e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3e86c42d50>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgNxvZRG1GQy"
      },
      "source": [
        "To summarize, in each state, train the neural network by following these steps:\n",
        "\n",
        "1. Choose an action using an epsilon-greedy policy, using the neural network to predict Q-values.\n",
        "1. Take the action and record the next state and reward.\n",
        "1. Calculate a target Q-value for the $(s,a)$ pair using the Bellman update.\n",
        "1. Train the neural network to predict the target Q-value.\n",
        "\n",
        "Over many transitions, your neural network will learn to approximate the Q-values for every state-action pair. Using these Q-values, the epsilon-greedy policy can solve the `FrozenLake-v0` environment. This approach is called **online DQN** because the agent trains on the state transitions generated when it is running (online)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGAcY2or2GgH"
      },
      "source": [
        "## Implement Framework to Solve Frozen Lake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_eGZasYqhmM"
      },
      "source": [
        "Define the functions you need to train your agent. Start by completing the following code cell to define a function that runs one training episode by repeating the steps described previously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H2MN-ItdvFJ"
      },
      "source": [
        "def collect_one_episode_and_train_model(env, model, epsilon, discount_factor):\n",
        "  '''Runs one episode and trains the model on every state transition.\n",
        "\n",
        "  Runs one episode. On every state transition in the episode, collects the\n",
        "  tuple s, a, r, s'. Then performs Bellman update on Q-values using the tuple\n",
        "  and trains the agent to predict the updated Q-values.\n",
        "\n",
        "  Args:\n",
        "    env: environment that the agent is learning.\n",
        "    model: neural network used to predict Q-values of (state, action) pairs\n",
        "    discount_factor: factor by which to reduce return from next state when\n",
        "      updating Q-values using Bellman update.\n",
        "  Returns:\n",
        "    episode_length: number of states visited during episode\n",
        "    episode_reward: total reward earned by agent during episode\n",
        "    model: updated model after training during episode\n",
        "  '''\n",
        "  state = env.reset()\n",
        "  episode_reward = 0\n",
        "  done = False\n",
        "  episode_length = 0\n",
        "\n",
        "  while not done:\n",
        "    episode_length += 1\n",
        "    # =========== Complete this section by replacing the \"...\" =================\n",
        "    q_values = model.predict(one_hot_encode_state(state))\n",
        "    action = policy_eps_greedy(env, q_values, epsilon)\n",
        "    state_new, reward, done, _ = env.step(action)\n",
        "    q_values[0, action] = bellman_update(reward, discount_factor, model,\n",
        "                                           state_new)\n",
        "    # ==========================================================================\n",
        "    model.fit(one_hot_encode_state(state), q_values, verbose=False)\n",
        "    episode_reward += reward\n",
        "    state = state_new\n",
        "\n",
        "  return(episode_length, episode_reward, model)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jT0xfzNqXii"
      },
      "source": [
        "#@title Solution (to view code, from cell's menu, select Form -> Show Code)\n",
        "def collect_one_episode_and_train_model(env, model, epsilon, discount_factor):\n",
        "  '''Runs one episode and trains the model on every state transition.\n",
        "\n",
        "  Runs one episode. On every state transition in the episode, collects the\n",
        "  tuple s, a, r, s'. Then performs Bellman update on Q-values using the tuple\n",
        "  and trains the agent to predict the updated Q-values.\n",
        "\n",
        "  Args:\n",
        "    env: environment that the agent is learning.\n",
        "    model: neural network used to predict Q-values of (state, action) pairs\n",
        "    discount_factor: factor by which to reduce return from next state when\n",
        "      updating Q-values using Bellman update.\n",
        "  Returns:\n",
        "    episode_length: number of states visited during episode\n",
        "    episode_reward: total reward earned by agent during episode\n",
        "    model: updated model after training during episode\n",
        "  '''\n",
        "  state = env.reset()\n",
        "  episode_reward = 0\n",
        "  done = False\n",
        "  episode_length = 0\n",
        "\n",
        "  while not done:\n",
        "    episode_length += 1\n",
        "    q_values = model.predict(one_hot_encode_state(state))\n",
        "    action = policy_eps_greedy(env, q_values, epsilon)\n",
        "    state_new, reward, done, _ = env.step(action)\n",
        "    q_values[0, action] = bellman_update(reward, discount_factor, model,\n",
        "                                         state_new)\n",
        "    model.fit(one_hot_encode_state(state), q_values, verbose=False)\n",
        "    episode_reward += reward\n",
        "    state = state_new\n",
        "\n",
        "  return(episode_length, episode_reward, model)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uouvJy-TrIbD"
      },
      "source": [
        "Define a function to test the agent's performance for a given success threshold. You will use this function to detect whether the agent has solved the enviroment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHLJZ4OIfw3B",
        "cellView": "both"
      },
      "source": [
        "def check_success(episode, reward_history, length_history, epsilon,\n",
        "                 success_percent_threshold):\n",
        "  '''Returns 1 if agent has crossed success threshold.\n",
        "\n",
        "  For a fixed number of episodes, calculates and prints metrics summarizing\n",
        "  agent's training over those episodes. Then checks and returns 1 if agent\n",
        "  has crossed the defined success threshold. Otherwise, returns 0.\n",
        "\n",
        "  Args:\n",
        "    episode: episode number of agent's training\n",
        "    reward_history: list containing rewards for every episode\n",
        "    length_history: list containing length of every episode, where length is\n",
        "      the number of states visited during the episode\n",
        "    epsilon: current value of epsilon\n",
        "    success_percent_threshold: percent of episodes that the agent must solve\n",
        "      to prove that it is successfully learning the environment\n",
        "  Returns:\n",
        "    1 if the agent crossed the success threshold, 0 otherwise.\n",
        "  '''\n",
        "  if((episode+1) % CHECK_SUCCESS_INTERVAL == 0):\n",
        "    # Check the success % in the last 100 episodes\n",
        "    success_percent = np.sum(reward_history[-100:-1])\n",
        "    length_avg = int(np.sum(length_history[-100:-1])/100.0)\n",
        "    print(\"Episode: \" + f\"{episode:0>4d}\" + \\\n",
        "          \", Success: \" + f\"{success_percent:2.0f}\" + \"%\" + \\\n",
        "          \", Avg length: \" + f\"{length_avg:0>2d}\" + \\\n",
        "          \", Epsilon: \" + f\"{epsilon:.2f}\")\n",
        "    if(success_percent > success_percent_threshold):\n",
        "      print(\"Agent crossed success threshold of \" + \\\n",
        "            str(success_percent_threshold) + '%.')\n",
        "      return(1)\n",
        "  return(0)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzKmi11OzaLF"
      },
      "source": [
        "Using the functions `collect_one_episode_and_train_model` and `check_success`, define a function to train the agent until the agent crosses the success threshold:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Scp6WgDEziQg"
      },
      "source": [
        "#### Plotting functions ####\n",
        "def visualize_training(reward_history):\n",
        "  plt.plot(range(len(reward_history)), reward_history)\n",
        "  plt.xlabel('Episodes')\n",
        "  plt.ylabel('Reward')\n",
        "  plt.title('Reward during Training')\n",
        "  plt.show()\n",
        "\n",
        "#### Training function ####\n",
        "def train_agent(env, model, episodes, epsilon, discount_factor, eps_decay,\n",
        "               success_percent_threshold):\n",
        "  '''Trains the agent by running episodes while checking for successful\n",
        "     learning.\n",
        "  Args:\n",
        "    env: environment to train the agent on\n",
        "    model: neural network representing agent used to learn Q-values of\n",
        "      environment\n",
        "    epsilon: starting value of epsilon\n",
        "    discount_factor: factor by which to reduce return from next state when\n",
        "      updating Q-values using Bellman update.\n",
        "    eps_decay: factor to reduce value of epsilon by, on every episode\n",
        "    episodes: number of episodes to train agent for\n",
        "    learning_rate: learning rate used by model\n",
        "  '''\n",
        "  length_history = []     # Record agent's episode length\n",
        "  reward_history = []     # Record agent's episode reward\n",
        "  timeStart = time.time() # Track training time\n",
        "\n",
        "  for episode in range(episodes):\n",
        "    episode_length, episode_reward, model = \\\n",
        "      collect_one_episode_and_train_model(env, model, epsilon, discount_factor)\n",
        "    length_history.append(episode_length)\n",
        "    reward_history.append(episode_reward)\n",
        "    if epsilon > EPSILON_MIN:\n",
        "      epsilon *= eps_decay\n",
        "    if(check_success(episode, reward_history, length_history, epsilon,\n",
        "                 success_percent_threshold)):\n",
        "      break\n",
        "\n",
        "  timeEnd = time.time()\n",
        "  print(\"Training time (min): \" + f'{(timeEnd - timeStart)/60:.2f}')\n",
        "  visualize_training(reward_history)\n",
        "  env.close() # Close environment"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKk1kXThRMpX"
      },
      "source": [
        "## Train Agent to Solve Frozen Lake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bQtaEcKv6nV"
      },
      "source": [
        "Run the code below to solve `FrozenLake-v0` using DQN. To solve Frozen Lake, you must play with hyperparameter values. In doing so, your goal is to develop intuition for how hyperparameters interact to affect the training outcome. \n",
        "\n",
        "Consider the following advice on adjusting hyperparameter values:\n",
        "\n",
        "* Journey length begins increasing before success rate. Hence, journey length is a leading indicator of improvement. Further, journey length is a more stable metric than success percent.\n",
        "* Aim to prioritize quick experimentation. For example, stop training if journey length doesn't begin increasing within 2000 episodes and try again.\n",
        "* The agent should solve the environment in <5000 episodes.\n",
        "* The output plot should show the incidence of successful episodes increasing.\n",
        "* Frozen Lake is slightly more complex than NChain. Adjust `learning_rate` accordingly. \n",
        "* The reward from the final state must propagate back to the initial state's Q-values. The higher the `discount_factor`, the greater the fraction of the reward that propagates back. Hence, keep `discount_factor` high.\n",
        "\n",
        "For the solution, expand the following section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxQr2GAMGD8B",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d41b270d-104b-4e46-f4a5-e7104f193eb3"
      },
      "source": [
        "##### SETUP #####\n",
        "episodes = 5000\n",
        "epsilon = 1.0\n",
        "eps_decay = 0.999\n",
        "learning_rate = 0.01\n",
        "discount_factor = 0.999\n",
        "success_percent_threshold = 20 # in percent, so 60 = 60%\n",
        "\n",
        "model = define_model(learning_rate)\n",
        "\n",
        "#### TRAINING #####\n",
        "train_agent(env, model, episodes, epsilon, discount_factor, eps_decay,\n",
        "               success_percent_threshold)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0099, Success:  0%, Avg length: 07, Epsilon: 0.90\n",
            "Episode: 0199, Success:  0%, Avg length: 07, Epsilon: 0.82\n",
            "Episode: 0299, Success:  1%, Avg length: 07, Epsilon: 0.74\n",
            "Episode: 0399, Success:  1%, Avg length: 07, Epsilon: 0.67\n",
            "Episode: 0499, Success:  1%, Avg length: 07, Epsilon: 0.61\n",
            "Episode: 0599, Success:  2%, Avg length: 09, Epsilon: 0.55\n",
            "Episode: 0699, Success:  2%, Avg length: 07, Epsilon: 0.50\n",
            "Episode: 0799, Success:  2%, Avg length: 08, Epsilon: 0.45\n",
            "Episode: 0899, Success:  3%, Avg length: 10, Epsilon: 0.41\n",
            "Episode: 0999, Success:  3%, Avg length: 08, Epsilon: 0.37\n",
            "Episode: 1099, Success:  0%, Avg length: 07, Epsilon: 0.33\n",
            "Episode: 1199, Success:  2%, Avg length: 07, Epsilon: 0.30\n",
            "Episode: 1299, Success:  0%, Avg length: 08, Epsilon: 0.27\n",
            "Episode: 1399, Success:  2%, Avg length: 08, Epsilon: 0.25\n",
            "Episode: 1499, Success:  2%, Avg length: 08, Epsilon: 0.22\n",
            "Episode: 1599, Success:  1%, Avg length: 08, Epsilon: 0.20\n",
            "Episode: 1699, Success:  0%, Avg length: 09, Epsilon: 0.18\n",
            "Episode: 1799, Success:  2%, Avg length: 07, Epsilon: 0.17\n",
            "Episode: 1899, Success:  0%, Avg length: 08, Epsilon: 0.15\n",
            "Episode: 1999, Success:  0%, Avg length: 08, Epsilon: 0.14\n",
            "Episode: 2099, Success:  1%, Avg length: 09, Epsilon: 0.12\n",
            "Episode: 2199, Success:  1%, Avg length: 08, Epsilon: 0.11\n",
            "Episode: 2299, Success:  0%, Avg length: 08, Epsilon: 0.10\n",
            "Episode: 2399, Success:  1%, Avg length: 08, Epsilon: 0.09\n",
            "Episode: 2499, Success:  0%, Avg length: 08, Epsilon: 0.08\n",
            "Episode: 2599, Success:  0%, Avg length: 07, Epsilon: 0.07\n",
            "Episode: 2699, Success:  0%, Avg length: 09, Epsilon: 0.07\n",
            "Episode: 2799, Success:  2%, Avg length: 08, Epsilon: 0.06\n",
            "Episode: 2899, Success:  0%, Avg length: 08, Epsilon: 0.05\n",
            "Episode: 2999, Success:  1%, Avg length: 08, Epsilon: 0.05\n",
            "Episode: 3099, Success:  0%, Avg length: 08, Epsilon: 0.04\n",
            "Episode: 3199, Success:  0%, Avg length: 09, Epsilon: 0.04\n",
            "Episode: 3299, Success:  1%, Avg length: 08, Epsilon: 0.04\n",
            "Episode: 3399, Success:  0%, Avg length: 08, Epsilon: 0.03\n",
            "Episode: 3499, Success:  0%, Avg length: 10, Epsilon: 0.03\n",
            "Episode: 3599, Success:  0%, Avg length: 08, Epsilon: 0.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_HCbNWf3RPP"
      },
      "source": [
        "## Solution (expand to view code)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZOrUw8d3R5K"
      },
      "source": [
        "The following code typically crosses a success rate of 20% in <2000 episodes. In the next cell, you'll visualize the trained agent solving the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0-Utan63TuQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "outputId": "cf6560ce-5f8a-48c6-9089-e358db03e729"
      },
      "source": [
        "##### SETUP #####\n",
        "episodes = 5000\n",
        "epsilon = 1.0\n",
        "eps_decay = 0.999\n",
        "learning_rate = 0.2\n",
        "discount_factor = 0.99\n",
        "success_percent_threshold = 60 # in percent, so 60 = 60%\n",
        "\n",
        "model = define_model(learning_rate)\n",
        "\n",
        "#### TRAINING #####\n",
        "train_agent(env, model, episodes, epsilon, discount_factor, eps_decay,\n",
        "               success_percent_threshold)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0099, Success:  3%, Avg length: 07, Epsilon: 0.90\n",
            "Episode: 0199, Success:  0%, Avg length: 08, Epsilon: 0.82\n",
            "Episode: 0299, Success:  3%, Avg length: 09, Epsilon: 0.74\n",
            "Episode: 0399, Success:  3%, Avg length: 08, Epsilon: 0.67\n",
            "Episode: 0499, Success:  3%, Avg length: 09, Epsilon: 0.61\n",
            "Episode: 0599, Success:  6%, Avg length: 11, Epsilon: 0.55\n",
            "Episode: 0699, Success:  7%, Avg length: 12, Epsilon: 0.50\n",
            "Episode: 0799, Success:  4%, Avg length: 12, Epsilon: 0.45\n",
            "Episode: 0899, Success:  6%, Avg length: 13, Epsilon: 0.41\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function WeakKeyDictionary.__init__.<locals>.remove at 0x7f3ea4e16560>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/weakref.py\", line 358, in remove\n",
            "    def remove(k, selfref=ref(self)):\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-d00536258dc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#### TRAINING #####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m train_agent(env, model, episodes, epsilon, discount_factor, eps_decay,\n\u001b[0;32m---> 13\u001b[0;31m                success_percent_threshold)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-eb1c066effb6>\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m(env, model, episodes, epsilon, discount_factor, eps_decay, success_percent_threshold)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mepisode_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m       \u001b[0mcollect_one_episode_and_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mlength_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mreward_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-ffc6692104e6>\u001b[0m in \u001b[0;36mcollect_one_episode_and_train_model\u001b[0;34m(env, model, epsilon, discount_factor)\u001b[0m\n\u001b[1;32m     29\u001b[0m     q_values[0, action] = bellman_update(reward, discount_factor, model,\n\u001b[1;32m     30\u001b[0m                                          state_new)\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot_encode_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1146\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1381\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func)\u001b[0m\n\u001b[1;32m   1901\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1902\u001b[0m     \"\"\"\n\u001b[0;32m-> 1903\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1905\u001b[0m   def interleave(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[1;32m   5061\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5062\u001b[0m     self._map_func = StructuredFunctionWrapper(\n\u001b[0;32m-> 5063\u001b[0;31m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0m\u001b[1;32m   5064\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5065\u001b[0m       raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   4216\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4218\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4219\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4220\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3149\u001b[0m     \"\"\"\n\u001b[1;32m   3150\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 3151\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   3152\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3153\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3114\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3115\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3116\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3117\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3118\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3306\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3307\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3308\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3309\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3310\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   4193\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   4194\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4195\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4196\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4197\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   4123\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4124\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4125\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4126\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4127\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_batch_indices\u001b[0;34m(indices)\u001b[0m\n\u001b[1;32m    311\u001b[0m         index_remainder = tf.data.Dataset.from_tensors(tf.slice(\n\u001b[1;32m    312\u001b[0m             indices, [num_in_full_batch], [self._partial_batch_size]))\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mflat_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_remainder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m   1140\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m     \"\"\"\n\u001b[0;32m-> 1142\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mConcatenateDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, dataset_to_concatenate)\u001b[0m\n\u001b[1;32m   4386\u001b[0m     variant_tensor = gen_dataset_ops.concatenate_dataset(\n\u001b[1;32m   4387\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_to_concatenate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4388\u001b[0;31m         **self._flat_structure)\n\u001b[0m\u001b[1;32m   4389\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4390\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConcatenateDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mconcatenate_dataset\u001b[0;34m(input_dataset, another_dataset, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m    888\u001b[0m                               \u001b[0manother_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manother_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m                               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m                               output_shapes=output_shapes, name=name)\n\u001b[0m\u001b[1;32m    891\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m       \u001b[0mattr_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_to_attr_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mattr_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"list(\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0m_SatisfiesLengthConstraint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_type_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mvalue_to_attr_value\u001b[0;34m(value, attr_type, arg_name)\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0mattr_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MakeShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mattr_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"list(shape)\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m     \u001b[0mattr_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_MakeShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mattr_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tensor\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[0mattr_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MakeTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0mattr_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MakeShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mattr_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"list(shape)\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m     \u001b[0mattr_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_MakeShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mattr_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tensor\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[0mattr_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MakeTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_MakeShape\u001b[0;34m(v, arg_name)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error converting %s to a TensorShape: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn-1xnkeHU1q"
      },
      "source": [
        "While Frozen Lake is a more complex environment than NChain, it is simple in comparison to environments such as Pong and Breakout. When solving more and more complex environments, apply the intuition gained from solving simpler environments by using the following guidelines:\n",
        "\n",
        "* The agent will take longer to find a successful path through random exploration. Therefore, epsilon must decay slower so that the agent explores for longer.\n",
        "* The agent must use a deeper and wider neural network to approximate the increased complexity.\n",
        "* The agent must train at a lower learning rate to adapt to the increased complexity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrYPnH2EGNRX"
      },
      "source": [
        "## Visualize Performance of Trained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiKRXasI6_NR"
      },
      "source": [
        "Seeing the metrics plots is one thing, but visualizing your agent succeed at retrieving the frisbee is another. Run the following code to visualize your agent solve `FrozenLake`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY9Gje9Q4N6J",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "635cf2b2-3534-42b1-ba45-c00973a36162"
      },
      "source": [
        "from IPython.display import clear_output # to clear output on every episode run\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "while(not(done)):\n",
        "  q_values = model.predict(np.identity(num_states)[state:state+1])\n",
        "  action = np.argmax(q_values)\n",
        "  state_new, reward, done,_ = env.step(action)\n",
        "  state = state_new\n",
        "  clear_output()\n",
        "  env.render()\n",
        "  time.sleep(0.5)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiv8fmR27U32"
      },
      "source": [
        "## Conclusion and Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xqjRz8e7Vdt"
      },
      "source": [
        "You learned how to combine neural networks with traditional reinforcement learning approaches to solve a simple environment.\n",
        "\n",
        "Move onto the next Colab: [Experience Replay and Target Networks](https://colab.research.google.com/drive/1DEv8FSjMvsgCDPlOGQrUFoJeAf67cFSo#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-experience-replay-and-target-networks).\n",
        "\n",
        "For reference, the sequence of course Colabs is as follows:\n",
        "\n",
        "1. [Problem Framing in Reinforcement Learning](https://colab.research.google.com/drive/1sUYro4ZyiHuuKfy6KXFSdWjNlb98ZROd#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-problem-framing)\n",
        "1. [Q-learning Framework](https://colab.research.google.com/drive/1ZPsEEu30SH1BUqUSxNsz0xeXL2Aalqfa#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-q-learning)\n",
        "1. [Tabular Q-Learning](https://colab.research.google.com/drive/1sX2kO_RA1DckhCwX25OqjUVBATmOLgs2#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-tabular-q-learning)\n",
        "1. [Deep Q-Learning](https://colab.research.google.com/drive/1XnFxIE882ptpO83mcAz7Zg8PxijJOsUs#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-deep-q-learning)\n",
        "1. [Experience Replay and Target Networks](https://colab.research.google.com/drive/1DEv8FSjMvsgCDPlOGQrUFoJeAf67cFSo#forceEdit=true&sandboxMode=true?utm_source=ss-reinforcement-learning&utm_campaign=colab-external&utm_medium=referral&utm_content=rl-experience-replay-and-target-networks)"
      ]
    }
  ]
}